{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><Bold>Movie Recommendation </Bold></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Packages Used</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '' #API Key\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "from random import random \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image \n",
    "import wordcloud as w\n",
    "from wordcloud import ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import folium\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "!pip install plotly\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Webscraping Data from TMDB</h1>\n",
    "<ul>Please do not run the below codes. Data files have been provided in the packet</ul>\n",
    "<ul>Please move on to the \"Combining Extacted Data\" code block</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. getting a list of genres\n",
    "# url_genre= 'https://api.themoviedb.org/3/genre/movie/list?api_key=51b58943e95f793242a8660c1da2acb7&language=en-US'\n",
    "# l=json.loads(requests.get(url_genre).content.decode('utf-8'))\n",
    "# g_id=[]\n",
    "# for i in l['genres']:\n",
    "#     g_id.append(i['id'])\n",
    "# genres_list=[]\n",
    "# id_list=[]\n",
    "# for i in l['genres']:\n",
    "#     id_list.append(i['id'])\n",
    "#     genres_list.append(i['name'])\n",
    "# genres=pd.DataFrame(data=list(zip(id_list,genres_list)),columns=['genre_id','genre'])\n",
    "# genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #code for scraping the data \n",
    "# #This has to be run only for scraping!!!! not otherwise \n",
    "# #using the 'discover' API of tmdb to get the list of movies \n",
    "# #Ran for 6 months at a time for efficiency purposes\n",
    "# url_movies='https://api.themoviedb.org/3/discover/movie?api_key=51b58943e95f793242a8660c1da2acb7&include_video=false&page=p&primary_release_date.gte=2018-10-01&primary_release_date.lte=2018-12-31'\n",
    "# #getting the results of page one\n",
    "# first_page=json.loads(requests.get(url_movies).content.decode('utf-8'))                       \n",
    "# results=[]\n",
    "# total_pages=first_page['total_pages'] #get the total pages so that we know how many times the loop has to be executed\n",
    "# print(total_pages)\n",
    "# for i in range(total_pages):\n",
    "#     print(i)\n",
    "#     sleep(random())\n",
    "#     page=json.loads(requests.get(url_movies+'&page=%s'%(i+1)).content.decode('utf-8')) \n",
    "#     r=page['results']\n",
    "#     for j in r:\n",
    "#         f=j['id']\n",
    "#         #print(f)\n",
    "#         sleep(random())\n",
    "#         url='https://api.themoviedb.org/3/movie/%s/credits?api_key=51b58943e95f793242a8660c1da2acb7' % f\n",
    "#         d=json.loads(requests.get(url).content.decode('utf-8'))\n",
    "#         sleep(random())\n",
    "#         url='https://api.themoviedb.org/3/movie/%s?api_key=51b58943e95f793242a8660c1da2acb7' % f\n",
    "#         d.update(json.loads(requests.get(url).content.decode('utf-8')))\n",
    "#         results.append(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combining Extracted Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF=open(\"movies.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df1= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df1)\n",
    "\n",
    "outF=open(\"movies_1.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df2= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df2)\n",
    "\n",
    "outF=open(\"movies_apr18_sep18_i-247.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df3= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df3)\n",
    "\n",
    "\n",
    "outF=open(\"movies_apr18_sep18_i-248_292.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df4= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df4)\n",
    "\n",
    "outF=open(\"movies_apr18_sep18_i-293_416.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df5= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df5)\n",
    "\n",
    "\n",
    "outF=open(\"first.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df6= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df6)\n",
    "\n",
    "outF=open(\"second.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df7= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df7)\n",
    "\n",
    "outF=open(\"third.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df8=pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df8)\n",
    "\n",
    "outF=open(\"fourth.txt\", \"r\") \n",
    "d=outF.readlines()\n",
    "df9= pd.DataFrame.from_dict(json.loads(d[0]), orient='columns')\n",
    "len(df9)\n",
    "\n",
    "#creating the final dataset\n",
    "\n",
    "df_merged1 = pd.concat([df1,df2], axis=0, sort = False)\n",
    "df_merged2 = pd.concat([df_merged1,df3], axis=0, sort = False)\n",
    "df_merged3 = pd.concat([df_merged2,df4], axis=0, sort = False)\n",
    "df_merged4 = pd.concat([df_merged3,df5], axis=0, sort = False)\n",
    "df_merged5 = pd.concat([df_merged4,df6], axis=0, sort = False)\n",
    "df_merged6 = pd.concat([df_merged5,df7], axis=0, sort = False)\n",
    "df_merged7 = pd.concat([df_merged6,df8], axis=0, sort = False)\n",
    "final_df = pd.concat([df_merged7,df9], axis=0, sort = False)\n",
    "\n",
    "#Reset the index\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Cleaning </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at Dataset\n",
    "# print(len(final_df.index))\n",
    "# print(final_df.columns)\n",
    "# print(final_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n",
    "def cleaning_data(df):\n",
    "    rows = len(df)\n",
    "    print(f'No. of rows in the original: {rows}')\n",
    "    #flag for all nas\n",
    "    df['flag_all_na'] = np.where(df['id'].isna() == True, 1, 0)\n",
    "    print(f'Rows dropped due to all Nas: {df.flag_all_na.sum()}')\n",
    "    #flag for empty genres\n",
    "    df['flag_genre'] = 0\n",
    "    for i in range(len(df['genres'])):\n",
    "        if df['flag_all_na'][i] != 1:\n",
    "            if len(df['genres'][i]) == 0:\n",
    "                df['flag_genre'][i] = 1\n",
    "    print(f'Rows dropped due to empty genres: {df.flag_genre.sum()}')\n",
    "    #flag for empty release_date\n",
    "    df['flag_date'] = np.where(df['release_date'] == '',1,0)\n",
    "    print(f'Rows dropped due to empty dates: {df.flag_date.sum()}')\n",
    "    #deleting the flagged rows\n",
    "    df = df[(df.flag_all_na == 0) & (df.flag_genre == 0) & (df.flag_date == 0)]\n",
    "    #keeping unique movie ids\n",
    "    df = df.drop_duplicates(subset='id')\n",
    "    #flag for duplicated movie titles\n",
    "    df.set_index('id')\n",
    "    dupes = list(df[df.duplicated(['title'])==True]['title'])\n",
    "    dupe_ids = []\n",
    "    keep_us = []\n",
    "    for i in dupes:\n",
    "        dupe_ids.extend(list(df[df['title']==i].index))\n",
    "        keep_us.append(df[df['title']==i]['vote_average'].idxmax())\n",
    "    remove_us = list(set(dupe_ids) - set(keep_us))\n",
    "    len_ = len(remove_us)\n",
    "    print(f'Rows dropped due to duplicate titles: {len_}')\n",
    "    df=df.drop(remove_us)\n",
    "    #creating month and year columns\n",
    "    df['Date_R'] = df['release_date'].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d'))\n",
    "    df['Year'] = df['Date_R'].apply(lambda x:x.year)\n",
    "    df['Month'] = df['Date_R'].apply(lambda x:x.month)\n",
    "    \n",
    "    #dropping the flag columns\n",
    "    df = df.drop(columns=['flag_all_na', 'flag_genre', 'flag_date'])\n",
    "    cleaned_row = len(df)\n",
    "    print(f'Final number of rows in our dataset:{cleaned_row}')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaning_data(final_df)\n",
    "cleaned_data.reset_index(inplace=True)\n",
    "cleaned_data.drop(columns=['level_0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at Cleaned Dataset\n",
    "# print(cleaned_data.index)\n",
    "# print(cleaned_data.columns)\n",
    "# print(cleaned_data.info())\n",
    "# final_rows = len(cleaned_data)\n",
    "# final_columns = len(cleaned_data.columns)\n",
    "# print(final_rows, final_columns)\n",
    "# print(len(cleaned_data.id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dimension Tables\n",
    "<ul>Will take some time to finish running</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating some dimension tables:\n",
    "#Cast of the movie\n",
    "#Genres\n",
    "#Director of the movie\n",
    "#Production country of the movie\n",
    "\n",
    "def dimension_table(df):\n",
    "    #creating cast dimension table\n",
    "    cast_list = []\n",
    "    for i in range(len(df)):\n",
    "        for d in df.iloc[i]['cast']:\n",
    "            cast_list.append([df.iloc[i]['id'],d['id'], d['name'], df.iloc[i]['vote_average']])\n",
    "    c_id_list = [i[0] for i in cast_list]\n",
    "    c_actor_id = [i[1] for i in cast_list]\n",
    "    c_actor_name = [i[2] for i in cast_list]\n",
    "    c_vote_average = [i[3] for i in cast_list]\n",
    "    if (len(c_id_list) == len(c_actor_id) == len(c_actor_name) == len(c_vote_average)) != True:\n",
    "        print('Error while creating cast dimension table. Please check.')\n",
    "    else:\n",
    "        cast_df = pd.DataFrame(columns = ['movie_id', 'actor_id', 'actor_name', 'vote_average'])\n",
    "        cast_df['movie_id'] = c_id_list\n",
    "        cast_df['actor_id'] = c_actor_id\n",
    "        cast_df['actor_name'] = c_actor_name\n",
    "        cast_df['vote_average'] = c_vote_average\n",
    "        \n",
    "    #creating genre dimension table\n",
    "    genre_list = []\n",
    "    for i in range(len(df)):\n",
    "        for d in df.iloc[i]['genres']:\n",
    "            genre_list.append([df.iloc[i]['id'],d['id'], d['name']]) \n",
    "    g_id_list = [i[0] for i in genre_list]\n",
    "    g_genre_id = [i[1] for i in genre_list]\n",
    "    g_genre_name = [i[2] for i in genre_list]\n",
    "    if (len(g_id_list) == len(g_genre_id) == len(g_genre_name)) != True:\n",
    "        print('Error while creating genre dimension table. Please check.')\n",
    "    else:\n",
    "        genre_df = pd.DataFrame(columns = ['movie_id', 'genre_id', 'genre_name'])\n",
    "        genre_df['movie_id'] = g_id_list\n",
    "        genre_df['genre_id'] = g_genre_id\n",
    "        genre_df['genre_name'] = g_genre_name\n",
    "        \n",
    "    #creating director dimension table\n",
    "    director_list = []\n",
    "    for i in range(len(df)):\n",
    "        for d in df.iloc[i]['crew']:\n",
    "            if d['job'] == 'Director':\n",
    "                director_list.append([df.iloc[i]['id'],d['name']])\n",
    "    d_id_list = [i[0] for i in director_list]\n",
    "    d_director_name = [i[1] for i in director_list]\n",
    "    if (len(d_id_list) == len(d_director_name)) !=True:\n",
    "        print('Error while creating director dimension table. Please check.')\n",
    "    else:\n",
    "        director_df = pd.DataFrame(columns = ['movie_id', 'director_name'])\n",
    "        director_df['movie_id'] = d_id_list\n",
    "        director_df['director_name'] = d_director_name\n",
    "    \n",
    "    #creating production country dimension table\n",
    "    prod_country_list = []\n",
    "    for i in range(len(df)):\n",
    "        for d in df.iloc[i]['production_countries']:\n",
    "            prod_country_list.append([df.iloc[i]['id'],d['name']])\n",
    "\n",
    "    p_id_list = [i[0] for i in prod_country_list]\n",
    "    p_prod_country = [i[1] for i in prod_country_list]\n",
    "    if (len(p_id_list) == len(p_prod_country)) != True:\n",
    "        print('Error while creating production country dimension table. Please check.')\n",
    "    else:\n",
    "        prod_country_df = pd.DataFrame(columns = ['movie_id', 'production_country'])\n",
    "        prod_country_df['movie_id'] = p_id_list\n",
    "        prod_country_df['production_country'] = p_prod_country\n",
    "        \n",
    "    #creating overview dimension\n",
    "    overview_df = df[['id', 'title', 'overview', 'genres']]\n",
    "    overview_df.set_index('id', inplace=True)\n",
    "    \n",
    "        \n",
    "    return cast_df, genre_df, director_df, prod_country_df, overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df, genre_df, director_df, prod_country_df, overview_df = dimension_table(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Visualizations</h1>\n",
    "<ul>\n",
    "    <li>3-D world map</li>\n",
    "    <li>Movie releases by month</li>\n",
    "    <li>Count by genre</li>\n",
    "    <li>Top 6 movies for top 6 genres</li>\n",
    "    <li>Word cloud</li>\n",
    "    <li>Revenue by genre</li>\n",
    "  </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Map\n",
    "df_code = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')\n",
    "df_code.at[211,'COUNTRY'] ='United States of America'\n",
    "df_code.at[108,'COUNTRY'] ='South Korea'\n",
    "df_code.at[73,'COUNTRY'] ='Gambia'\n",
    "df_code.at[32,'COUNTRY'] = 'Myanmar'\n",
    "df_code.at[121,'COUNTRY']='Macao'\n",
    "df_code.at[112,'COUNTRY']='Lao People\\'s Democratic Republic'\n",
    "df_code.at[111,'COUNTRY']= 'Kyrgyz Republic'\n",
    "country_geo = 'world-countries.json'\n",
    "df_map = pd.DataFrame(prod_country_df.groupby('production_country').size())\n",
    "df_map = df_map.rename(columns={0:'Count'})\n",
    "df_map = df_map.reset_index()\n",
    "df_map_final=df_code.merge(df_map, left_on='COUNTRY', right_on='production_country')[['production_country','Count','CODE']]\n",
    "\n",
    "df_map_final['text'] = df_map_final['production_country']\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=df_map_final['CODE'], # Spatial coordinates\n",
    "    z = df_map_final['Count'], # Data to be color-coded\n",
    "    text = df_map_final['text'],\n",
    "    colorscale = 'Reds',\n",
    "    autocolorscale=False,\n",
    "    reversescale=False,\n",
    "    marker_line_color='darkgray',\n",
    "    marker_line_width=0.5,\n",
    "    colorbar_title = \"Movies\",\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Movies',\n",
    "    geo=dict(\n",
    "        showframe=False,\n",
    "        showcoastlines=False,\n",
    "        projection_type='orthographic'\n",
    "    ),\n",
    "    annotations = [dict(\n",
    "        x=0.55,\n",
    "        y=0.1,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        text= 'Data Source: Tmdb',\n",
    "        showarrow = False\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#saving the plot offline \n",
    "# from plotly.offline import plot    \n",
    "# plot(fig, \"file.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Month-Year Plot for count of movies released\n",
    "cleaned_data.groupby(['Year', 'Month']).size().plot(title='Releases by Month', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Genres bar chart\n",
    "graph0 = genre_df.groupby('genre_name').size().sort_values(ascending=False).plot(kind='bar',title='Released Movies by Genre', color='k')\n",
    "fig = graph0.get_figure()\n",
    "fig.savefig(\"graph0.png\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. top 6 movies (by rating) for top 6 genres from above chart\n",
    "\n",
    "df_plot_genre= cleaned_data.merge(genre_df, left_on='id', right_on='movie_id').merge(prod_country_df,left_on = 'id', right_on='movie_id')\n",
    "df_plot_genre = df_plot_genre[df_plot_genre.vote_count > 10]\n",
    "df_plot_genre = df_plot_genre[df_plot_genre.production_country == 'United States of America']\n",
    "\n",
    "df_plot_drama = df_plot_genre[df_plot_genre.genre_name == 'Drama'].sort_values(by='vote_average', ascending=False)\n",
    "df_plot_drama = df_plot_drama[0:6][['title','vote_average']]\n",
    "\n",
    "df_plot_docu = df_plot_genre[df_plot_genre.genre_name == 'Documentary'].sort_values(by='vote_average', ascending=False)\n",
    "df_plot_docu = df_plot_docu[0:6][['title','vote_average']]\n",
    "\n",
    "df_plot_comedy = df_plot_genre[df_plot_genre.genre_name == 'Comedy'].sort_values(by='vote_average', ascending=False)\n",
    "df_plot_comedy = df_plot_comedy[0:6][['title','vote_average']]\n",
    "\n",
    "df_plot_horror = df_plot_genre[df_plot_genre.genre_name == 'Horror'].sort_values(by='vote_average', ascending=False)\n",
    "df_plot_horror = df_plot_horror[0:6][['title','vote_average']]\n",
    "\n",
    "df_plot_thriller = df_plot_genre[df_plot_genre.genre_name == 'Thriller'].sort_values(by='vote_average', ascending=False)\n",
    "df_plot_thriller = df_plot_thriller[0:6][['title','vote_average']]\n",
    "\n",
    "graph1 = df_plot_drama.sort_values(by='vote_average', ascending=True).plot(kind='barh', x='title', title='Drama', color='g')\n",
    "fig = graph1.get_figure()\n",
    "fig.savefig(\"graph1.png\",bbox_inches='tight')\n",
    "graph2 = df_plot_docu.sort_values(by='vote_average', ascending=True).plot(kind='barh', x='title', title='Documentary', color='b')\n",
    "fig = graph2.get_figure()\n",
    "fig.savefig(\"graph2.png\",bbox_inches='tight')\n",
    "\n",
    "graph3 = df_plot_comedy.sort_values(by='vote_average', ascending=True).plot(kind='barh', x='title', title='Comedy', color='r')\n",
    "fig = graph3.get_figure()\n",
    "fig.savefig(\"graph3.png\",bbox_inches='tight')\n",
    "\n",
    "graph4 = df_plot_horror.sort_values(by='vote_average', ascending=True).plot(kind='barh', x='title', title='Horror', color='y')\n",
    "fig = graph4.get_figure()\n",
    "fig.savefig(\"graph4.png\",bbox_inches='tight')\n",
    "\n",
    "graph5 = df_plot_thriller.sort_values(by='vote_average', ascending=True).plot(kind='barh', x='title', title='Thriller', color='b')\n",
    "fig = graph5.get_figure()\n",
    "fig.savefig(\"graph5.png\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Word Cloud\n",
    "g_id = list(genre_df.genre_id.unique())\n",
    "#We want to extract the movie ids for each genres\n",
    "m_g_list=[]\n",
    "for i in g_id: #g_id is extracted from genres API\n",
    "    m_id=[]\n",
    "    for j in range(len(genre_df['genre_id'])):\n",
    "        if i == genre_df['genre_id'][j]:\n",
    "            m_id.append(genre_df['movie_id'][j])\n",
    "    m_g_list.append(m_id)\n",
    "\n",
    "#getting overviews by genre\n",
    "overview_list = {} \n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "count=0\n",
    "for i in range(len(m_g_list)):\n",
    "    overview_g=''\n",
    "    for j in m_g_list[i]:\n",
    "        overview_g+=overview_df['overview'][j]\n",
    "    overview_list[g_id[count]] = regex.sub(\"\", overview_g).lower()\n",
    "    count+=1\n",
    "#     print('count',count)\n",
    "    \n",
    "def shaped_wordcloud(g_id, pic):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stopword_list = stopwords.words('english')    \n",
    "    tokens = nltk.word_tokenize(overview_list[g_id])    \n",
    "    tokens = [token.strip() for token in tokens]    \n",
    "    text=[token for token in tokens if token not in stopword_list]\n",
    "    lemma=' '.join(wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text)\n",
    "    char_mask = np.array(Image.open(pic))\n",
    "    image_colors = ImageColorGenerator(char_mask)\n",
    "    wc = w.WordCloud(background_color=\"white\",max_font_size=1000,max_words= 100,width=800,height=400,mask=char_mask, random_state=1).generate(lemma)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wc.recolor(color_func=image_colors),interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a. Romance\n",
    "shaped_wordcloud(10749,\"heart.jpeg\")\n",
    "#4b. Action\n",
    "shaped_wordcloud(28,\"action.jpeg\")\n",
    "#4c. Family\n",
    "shaped_wordcloud(10751,\"family.JPG\")\n",
    "#4d. Horror\n",
    "shaped_wordcloud(27,\"horror.jpg\")\n",
    "#4e. Crime\n",
    "shaped_wordcloud(80,\"crime_1.jpg\")\n",
    "#4f. Animation\n",
    "shaped_wordcloud(16,\"animation.jpg\")\n",
    "#4g. Adventure\n",
    "shaped_wordcloud(12,\"adventure.jpg\")\n",
    "#4h. Music\n",
    "shaped_wordcloud(10402,\"music.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Box plot for revenues by genre\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "rev_list=[]\n",
    "for i in range(len(genre_df)):\n",
    "    temp_id = genre_df.iloc[i]['movie_id']\n",
    "    rev_list.append(cleaned_data[cleaned_data['id']==temp_id]['revenue'].item())\n",
    "genre_rev_df = pd.DataFrame()\n",
    "genre_rev_df['movie_id']= genre_df['movie_id']\n",
    "genre_rev_df['genre'] = genre_df['genre_name']\n",
    "genre_rev_df['revenue']= rev_list\n",
    "genre_rev_df_1 = genre_rev_df[genre_rev_df.revenue != 0.0]\n",
    "fig, ax = plt.subplots(figsize =(20, 10)) \n",
    "chart =sns.boxplot(ax=ax,data = genre_rev_df_1, x = genre_rev_df_1[\"genre\"],y = genre_rev_df_1[\"revenue\"])\n",
    "labels =chart.get_xticklabels()\n",
    "chart.set_xticklabels(labels,rotation=45)\n",
    "fig = chart.get_figure()\n",
    "fig.savefig(\"graph.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Model 1 : Shortest Path</strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final tweaks\n",
    "movie_genre_df = genre_df.groupby(['movie_id', 'genre_name']).size().unstack()\n",
    "movie_genre_df = movie_genre_df.fillna(0)\n",
    "\n",
    "def create_network_dictionary(ids):\n",
    "    genre_list = list(genre_df[genre_df.movie_id == ids]['genre_name'])\n",
    "    director_list = list(director_df[director_df.movie_id == ids]['director_name'])\n",
    "    cont_list = list(prod_country_df[prod_country_df.movie_id ==ids]['production_country'])\n",
    "    sample_network = {}\n",
    "    sample_director = {}\n",
    "    sample_country={}\n",
    "    \n",
    "    for row1 in genre_df.itertuples(index=False):\n",
    "        if row1[2] in genre_list and ids != row1[0]:\n",
    "            if (ids, row1[0]) not in sample_network:\n",
    "                sample_network[(ids, row1[0])] = 1\n",
    "            else:\n",
    "                sample_network[(ids, row1[0])] += 1\n",
    "        \n",
    "    for row1 in director_df.itertuples(index=False):\n",
    "        if row1[1] in director_list and ids != row1[0]:\n",
    "            if (ids, row1[0]) not in sample_director:\n",
    "                sample_director[(ids, row1[0])] = 1\n",
    "            else:\n",
    "                sample_director[(ids, row1[0])] += 1  \n",
    "                \n",
    "     \n",
    "    for row1 in prod_country_df.itertuples(index=False):\n",
    "        if row1[1] in cont_list and ids != row1[0]:\n",
    "            if (ids, row1[0]) not in sample_country:\n",
    "                sample_country[(ids, row1[0])] = 1\n",
    "            else:\n",
    "                sample_country[(ids, row1[0])] += 1  \n",
    "                \n",
    "    for (key1,key2),value in sample_network.items():\n",
    "        a = movie_genre_df.loc[[key1, key2],].sum()\n",
    "        a = np.where(a>=1,1,0)\n",
    "        counts = a.sum()\n",
    "        sample_network[key1, key2] = (value/counts)*0.60\n",
    "        \n",
    "    for (key1,key2),value in sample_network.items():\n",
    "        if (key1,key2) in sample_director:\n",
    "            sample_network[key1, key2] = value + 0.20\n",
    "        if (key1,key2)in sample_country:\n",
    "            sample_network[key1, key2] = value + 0.20\n",
    "            \n",
    "    for (key1,key2),value in sample_network.items():\n",
    "        sample_network[key1, key2] = 1 - value\n",
    "   \n",
    "    return sample_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_model(df):\n",
    "    print(\"Find Recommendations Based on Movie Input\")\n",
    "    x=0\n",
    "    while x<4:\n",
    "        movie = input(\"Type name of a movie you liked: \" )\n",
    "        sleep(1)\n",
    "        try:\n",
    "            if movie.lower() in list(map(lambda x:x.lower(),list(df['title']))):\n",
    "                print(\"Getting your recommendations\")\n",
    "            row = df.loc[df['title'].str.lower()==movie.lower()]\n",
    "            ids = np.array(row['id'])[0]\n",
    "            network_dictionary = create_network_dictionary(ids)\n",
    "\n",
    "            dict_rec = {}\n",
    "            for key,value in network_dictionary.items():\n",
    "                dict_rec[key[1]] = value\n",
    "            dict_rec = sorted(dict_rec.items(), key=lambda x:x[1], reverse=False)\n",
    "            Recommended_movies = []\n",
    "            for x in dict_rec[0:5]:\n",
    "                Recommended_movies.extend(np.array(df.loc[df['id']==x[0]]['title']))\n",
    "\n",
    "            return Recommended_movies\n",
    "        except:\n",
    "            x +=1\n",
    "            print(\"\"\"We dont recognize that movie, make sure that: \n",
    "                (1) the name is correctly spelled and;\n",
    "                (2) the movie was released in past 2 years\"\"\")\n",
    "            continue\n",
    "    else:\n",
    "        print('Exceeded the limit of 4 attempts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run the shortest path model, please uncomment the next code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest_path_model(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model 2</h2>\n",
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['genres','crew','overview']\n",
    "\n",
    "title_dict = {}\n",
    "overview_dict ={}\n",
    "genres_dict={}\n",
    "director_dict={}\n",
    "\n",
    "for i in range(len(cleaned_data)):\n",
    "    temp_id = cleaned_data.iloc[i]['id']\n",
    "    \n",
    "    #overview\n",
    "    overview_dict[temp_id] = cleaned_data.iloc[i]['overview']\n",
    "    \n",
    "    #genres\n",
    "    genre =''\n",
    "    for d in cleaned_data.iloc[i]['genres']:\n",
    "        genre = genre+ \" \"+ d['name']   \n",
    "    genres_dict[temp_id] =genre\n",
    "    \n",
    "    #directors\n",
    "    director =''\n",
    "    for d in cleaned_data.iloc[i]['crew']:\n",
    "        if d['job'] == 'Director':\n",
    "            director = director+ \" \"+ d['name']\n",
    "    director_dict[temp_id] =director\n",
    "    \n",
    "    #title\n",
    "    title_dict[temp_id] = cleaned_data.iloc[i]['title']\n",
    "    \n",
    "string_overview_df= pd.DataFrame.from_dict(overview_dict, orient='index', columns = ['overview'])\n",
    "string_genre_df= pd.DataFrame.from_dict(genres_dict, orient='index', columns = ['genres'])\n",
    "string_director_df= pd.DataFrame.from_dict(director_dict, orient='index', columns = ['director'])   \n",
    "title_df= pd.DataFrame.from_dict(title_dict, orient='index', columns = ['title'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_df = string_genre_df.join(string_director_df).join(title_df).join(string_overview_df)\n",
    "cosine_df.reset_index(inplace=True)\n",
    "cosine_df.reset_index(inplace=True)\n",
    "cosine_df = cosine_df.rename(columns={'index':'movie_id', 'level_0':'index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_from_index(index):\n",
    "    return cosine_df[cosine_df.index == index][\"title\"].values[0]\n",
    "\n",
    "def get_index_from_title(title):\n",
    "    return cosine_df[cosine_df['title'] == title].index[0]\n",
    "\n",
    "def combine_features(row):\n",
    "    try:\n",
    "        return row[\"genres\"]+\" \"+row[\"director\"]+\" \"+row['overview']\n",
    "    except:\n",
    "        print ('Error:', row)\n",
    "        \n",
    "def filtering(row):\n",
    "    if len(row[\"genres\"])>0 and len(row[\"director\"])>0 and len(row[\"overview\"])>0:\n",
    "        return row\n",
    "    else:\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_movie_title_model(df):\n",
    "\n",
    "    movie_user_likes = input(\"Type name of movie you liked: \" )\n",
    "    try:\n",
    "        if movie_user_likes.lower() in list(map(lambda x:x.lower(),list(df['title']))):\n",
    "            print(\"Getting your recommendations\")\n",
    "\n",
    "    ##Step 1: Filter out data that has all non-empty 'feature' columns    \n",
    "        result_df= df.apply(filtering,axis=1)\n",
    "\n",
    "##Movie for which user needs recommendation    \n",
    "    ##Step 2: Select Features\n",
    "        features = ['genres','director','overview']\n",
    "\n",
    "    ##Step 3: Create a column in DF which combines all selected features\n",
    "        for feature in features:\n",
    "            result_df[feature] = result_df[feature].fillna('')    \n",
    "\n",
    "    ##Step 4: 'combined_features' column has the combined string \n",
    "        result_df[\"combined_features\"] = result_df.apply(combine_features,axis=1)\n",
    "\n",
    "    ##Step 5: Create count matrix from this new combined column\n",
    "        cv = CountVectorizer()\n",
    "        count_matrix = cv.fit_transform(result_df[\"combined_features\"])\n",
    "\n",
    "    ##Step 6: Compute the Cosine Similarity based on the count_matrix\n",
    "        cosine_sim = cosine_similarity(count_matrix) \n",
    "\n",
    "    ##Step 7: Get index of this movie from its title\n",
    "        movie_index = get_index_from_title(movie_user_likes)\n",
    "        similar_movies =  list(enumerate(cosine_sim[movie_index]))\n",
    "\n",
    "    ##Step 8: Get a list of similar movies in descending order of similarity score\n",
    "        sorted_similar_movies = sorted(similar_movies,key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    ##Step 9: Print titles of first 50 movies\n",
    "        i=0\n",
    "        for element in sorted_similar_movies:\n",
    "            print (get_title_from_index(element[0]))\n",
    "            i=i+1\n",
    "            if i>5:\n",
    "                break\n",
    "    except:\n",
    "        print(\"\"\"We dont recognize that movie, make sure that: \n",
    "                (1) the name is correctly spelled and;\n",
    "                (2) the movie was released in past 2 years\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run the cosine model, please uncomment the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_movie_title_model(cosine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model 3 Keyword Search</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=overview_df.copy()\n",
    "def get_clean_overview():\n",
    "    overview_index=df['overview'].index\n",
    "    id_overview=[]\n",
    "    for i in range(len(df['overview'])):\n",
    "        id_overview.append([overview_index[i],df['overview'][overview_index[i]]])\n",
    "    #removing punctuations and making them lowercase \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for i in id_overview:\n",
    "        if i[1]:\n",
    "            i[1]=regex.sub(\"\",i[1]).lower()\n",
    "            i[1]=re.sub(r'[^\\x00-\\x7F]+','',i[1]) #removing non ascii characters\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stopword_list = stopwords.words('english')    \n",
    "    for i in id_overview:\n",
    "        if i[1]: \n",
    "            tokens = nltk.word_tokenize(i[1])    \n",
    "            tokens = [token.strip() for token in tokens]    \n",
    "            text=[token for token in tokens if token not in stopword_list]\n",
    "            lemma=[wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text]\n",
    "            i[1]=lemma\n",
    "    #now we make a dictionary of each overview, i.e. each word with an id \n",
    "    single_overview=[]\n",
    "    for i in id_overview:\n",
    "        if i[1]:\n",
    "            single_overview.append(i[1])\n",
    "    titles=[]\n",
    "    for i in id_overview:\n",
    "        if i[1]:\n",
    "            titles.append(df.title[i[0]])\n",
    "    return single_overview,titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make the corpora, titles for all movies in the data set that have a genre \n",
    "def make_corpora():\n",
    "    single_overview,titles = get_clean_overview()\n",
    "    dictionary = Dictionary(single_overview)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in single_overview] #creating a corpus or bag of words for each movie\n",
    "    from gensim.models.tfidfmodel import TfidfModel\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    from gensim.similarities import MatrixSimilarity\n",
    "    sims = MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
    "    \n",
    "    return tfidf, sims, titles, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create a function to accept search words and then show the top 4 similar movies using the tfidf model\n",
    "def tf_idf_model(search_words):\n",
    "    m_overview=search_words\n",
    "    tfidf, sims, titles,dictionary= make_corpora() #make corpus of all movies\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    m_overview = regex.sub(\"\",m_overview).lower()\n",
    "    m_overview=re.sub(r'[^\\x00-\\x7F]+','',m_overview)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stopword_list = stopwords.words('english')\n",
    "    #m_overview=m_overview.split(' ')\n",
    "    tokens = nltk.word_tokenize(m_overview)    \n",
    "    tokens = [token.strip() for token in tokens]    \n",
    "    text=[token for token in tokens if token not in stopword_list]\n",
    "    lemma=[wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text]\n",
    "    m_overview=lemma\n",
    "    \n",
    "    doc_bow = dictionary.doc2bow(m_overview)\n",
    "    doc_tfidf = tfidf[doc_bow]\n",
    "    similarity_array = sims[doc_tfidf]\n",
    "   \n",
    "    similarity_series = pd.Series(similarity_array.tolist(), index=titles)\n",
    "    top_hits = similarity_series.sort_values(ascending=False)[0:5]\n",
    "    \n",
    "    return top_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run tf_idf model, please uncomment the below code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_model('snow christmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_keyword_model_part1(search_words):\n",
    "    list_of_strings=[]\n",
    "    single_overview, titles = get_clean_overview()\n",
    "    for i in single_overview:\n",
    "        list_of_strings.append(','.join(i)) #each string in the list is overview for a single movie\n",
    "    list_of_strings.insert(0,search_words) #adding the search words to get the similarity row for these with movies\n",
    "    #print(list_of_strings)\n",
    "    cv = CountVectorizer(list_of_strings)\n",
    "    count_matrix = cv.fit_transform(list_of_strings).toarray()\n",
    "    cosine_sim = cosine_similarity(count_matrix)\n",
    "    sim_d={}\n",
    "    for i in range(len(cosine_sim[0])):\n",
    "        sim_d[i]=cosine_sim[0][i] #extracting the row of search words and putting into dictionary\n",
    "    sorted_similar_movies = sorted(sim_d.items(),key=lambda x:x[1],reverse=True)[1:6]\n",
    "    recommendation=[]\n",
    "    for i in sorted_similar_movies:\n",
    "        recommendation.append((titles[i[0]-1],i[1]))\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "\n",
    "def get_clean_overview_for_genre(g_id):\n",
    "    overview_index=df['overview'].index\n",
    "    id_overview_genre=[]\n",
    "    for i in range(len(df['overview'])):\n",
    "        id_overview_genre.append([overview_index[i],df['overview'][overview_index[i]],df['genres'][overview_index[i]]]) \n",
    "    for i in id_overview_genre:\n",
    "        l=[]\n",
    "        for j in i[2]:\n",
    "            l.append(j['id'])\n",
    "        i[2]=l\n",
    "    id_overview_genre_specific=[]\n",
    "    for i in id_overview_genre:\n",
    "        if g_id in i[2]:\n",
    "            id_overview_genre_specific.append(i)\n",
    "    \n",
    "    \n",
    "    #removing punctuations and making them lowercase \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for i in id_overview_genre_specific:\n",
    "        if i[1]:\n",
    "            i[1]=regex.sub(\"\",i[1]).lower()\n",
    "            i[1]=re.sub(r'[^\\x00-\\x7F]+','',i[1]) #removing non ascii characters\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stopword_list = stopwords.words('english')    \n",
    "    for i in id_overview_genre_specific:\n",
    "        if i[1]: \n",
    "            tokens = nltk.word_tokenize(i[1])    \n",
    "            tokens = [token.strip() for token in tokens]    \n",
    "            text=[token for token in tokens if token not in stopword_list]\n",
    "            lemma=[wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text]\n",
    "            i[1]=lemma\n",
    "    \n",
    "    single_overview=[]\n",
    "    for i in id_overview_genre_specific:\n",
    "        if i[1]:\n",
    "            single_overview.append(i[1])\n",
    "    titles=[]\n",
    "    for i in id_overview_genre_specific: #getting movie titles \n",
    "        if i[1]:\n",
    "            titles.append(df.title[i[0]])\n",
    "    return single_overview, titles\n",
    "\n",
    "\n",
    "def cosine_keyword_model_part2(search_words,g_id):\n",
    "    list_of_strings=[]\n",
    "    single_overview, titles = get_clean_overview_for_genre(g_id)\n",
    "    for i in single_overview:\n",
    "        list_of_strings.append(','.join(i)) #each string in the list is overview for a single movie\n",
    "    list_of_strings.insert(0,search_words) #adding the search words to get the similarity row for these with movies\n",
    "    #print(list_of_strings)\n",
    "    cv = CountVectorizer(list_of_strings)\n",
    "    count_matrix = cv.fit_transform(list_of_strings).toarray()\n",
    "    cosine_sim = cosine_similarity(count_matrix)\n",
    "    sim_d={}\n",
    "    for i in range(len(cosine_sim[0])):\n",
    "        sim_d[i]=cosine_sim[0][i] #extracting the row of search words and putting into dictionary\n",
    "    sorted_similar_movies = sorted(sim_d.items(),key=lambda x:x[1],reverse=True)[1:6]\n",
    "    recommendation=[]\n",
    "    for i in sorted_similar_movies:\n",
    "        recommendation.append((titles[i[0]-1],i[1]))\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "def cosine_keyword_model_combined(search_words,g_id=0):\n",
    "    if g_id==0:\n",
    "        movies =cosine_keyword_model_part1(search_words)\n",
    "    else:\n",
    "        movies =cosine_keyword_model_part2(search_words,g_id)\n",
    "    return movies\n",
    "\n",
    "\n",
    "\n",
    "def cosine_keyword_model():\n",
    "    genres = genre_df[['genre_id', 'genre_name']].drop_duplicates(subset='genre_id')\n",
    "    genres = genres.set_index('genre_id')\n",
    "    print('You can search by only keywords or keywords & genre')\n",
    "    sleep(1)\n",
    "    print('Please note if you choose to search without limiting on genres, the model will take longer to return results.')\n",
    "    sleep(1)\n",
    "    ans=input('Do you wish to search by genre [Y/N]:')\n",
    "    if ans.lower() == 'y':\n",
    "        print('Here is the genre list')\n",
    "        print(genres)\n",
    "        gen_id= input('Enter your selected genre id:')\n",
    "        gen_id= int(gen_id)\n",
    "        try:\n",
    "            selected_genre=genres[genres.index==int(gen_id)].iloc[0]\n",
    "            print('You have selected',selected_genre[0])\n",
    "            sleep(1)\n",
    "            search_words=input('Enter the keywords you want to search with (e.g. snow love):')\n",
    "            print('Getting your recommendations...')\n",
    "            movies= cosine_keyword_model_combined(search_words,gen_id)\n",
    "            movies_1 = []\n",
    "            for i in movies:\n",
    "                movies_1.append(i[0]) \n",
    "        except IndexError:\n",
    "            print('Selected genre does not exist!')\n",
    "    elif ans.lower() == 'n':\n",
    "        search_words=input('Enter the keywords you want to search with:')\n",
    "        print('Getting your recommendations...')\n",
    "        movies= cosine_keyword_model_combined(search_words)\n",
    "        movies_1 = []\n",
    "        for i in movies:\n",
    "            movies_1.append(i[0])\n",
    "    else:\n",
    "        print('Invalid entry. Please run the model again.')\n",
    "    return movies_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run cosine keyword model, please uncomment the below code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_keyword_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Final Project Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SME_recommendation_tool():\n",
    "    print('Its movie time! Lets save 51 minutes of your life!')\n",
    "    sleep(1)\n",
    "    search_input = input('Would you like to search by movie title [1] or keywords[2]? Please enter your option number:')\n",
    "    if search_input == '1':\n",
    "        return shortest_path_model(cleaned_data)\n",
    "    elif search_input == '2':\n",
    "        return cosine_keyword_model()\n",
    "    else:\n",
    "        print('Invalid entry, try again.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SME_recommendation_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
